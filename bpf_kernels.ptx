// =============================================================================
// bpf_kernels.ptx -- Bootstrap Particle Filter in raw PTX
// Target: SM_120 (Blackwell consumer / RTX 5080, 5090)
//
// Educational implementation showing the full BPF pipeline:
//   1. bpf_propagate_weight  -- SV observation log-likelihood weights
//   2. bpf_set_scalar        -- Initialize a device scalar
//   3. bpf_reduce_max        -- Block-parallel max reduction -> atomicMax
//   4. bpf_reduce_sum        -- Block-parallel sum reduction -> atomicAdd
//   5. bpf_exp_sub           -- w[i] = exp(log_w[i] - max_lw)
//   6. bpf_scale_wh          -- Normalize weights, compute w*h product
//   7. bpf_compute_loglik    -- log(sum_w / N) + max_lw
//   8. bpf_resample          -- Systematic resampling via binary search on CDF
//
// Compile:
//   ptxas -arch=sm_120 -o bpf_kernels.cubin bpf_kernels.ptx
//
// Load at runtime with cuModuleLoad / cuModuleGetFunction.
//
// NOTE: This PTX omits RNG/propagation noise for clarity -- the propagate kernel
// shows the weighting math. In production you'd integrate cuRAND device API
// or implement your own PRNG in PTX (e.g., PCG32/xoshiro).
//
// All kernels use 256-thread blocks (8 warps), matching the CUDA version.
// =============================================================================

.version 8.8          // PTX ISA 8.8 (CUDA 13.1)
.target sm_120        // Blackwell consumer (RTX 5080/5090)
.address_size 64

// =============================================================================
// Kernel 1: bpf_propagate_weight
//
// For each particle i:
//   eta = y_t * exp(-h[i] * 0.5)
//   log_w[i] = -0.9189385 - 0.5 * eta² - h[i] * 0.5    (Gaussian obs)
//
// Params:
//   param_h     = h (float*)
//   param_log_w = log_w (float*)
//   param_y_t   = y_t (float)
//   param_n     = n (int)
// =============================================================================

.visible .entry bpf_propagate_weight(
    .param .u64 param_h,
    .param .u64 param_log_w,
    .param .f32 param_y_t,
    .param .s32 param_n
)
{
    .reg .u32   %idx, %r_tid, %r_ctaid, %r_ntid;
    .reg .u64   %rd_h, %rd_lw;
    .reg .u64   %rd_off;
    .reg .u32   %r_n;
    .reg .f32   %f_yt, %f_hi, %f_eta, %f_eta2;
    .reg .f32   %f_half_h, %f_log_w, %f_neg_half, %f_log2pi;
    .reg .pred  %p_inbound;

    ld.param.u64    %rd_h,    [param_h];
    ld.param.u64    %rd_lw,   [param_log_w];
    ld.param.f32    %f_yt,    [param_y_t];
    ld.param.s32    %r_n,     [param_n];

    // Global thread index: idx = blockIdx.x * blockDim.x + threadIdx.x
    // Special registers must be mov'd to GPRs before use as operands
    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    // Bounds check
    setp.ge.u32     %p_inbound, %idx, %r_n;
    @%p_inbound bra PROP_EXIT;

    // Byte offset: idx * 4
    mul.wide.u32    %rd_off,  %idx, 4;

    // Load h[i]
    add.u64         %rd_h,    %rd_h, %rd_off;
    ld.global.f32   %f_hi,    [%rd_h];

    // eta = y_t * exp(-h[i] * 0.5)
    mov.f32         %f_neg_half, 0fBF000000;       // -0.5f
    mul.f32         %f_half_h,   %f_hi, %f_neg_half;
    // exp(x) = exp2(x * log2(e)),  log2(e) = 1.4426950f
    mul.f32         %f_half_h,   %f_half_h, 0f3FB8AA3B;
    ex2.approx.f32  %f_half_h,   %f_half_h;
    mul.f32         %f_eta,      %f_yt, %f_half_h;

    // eta²
    mul.f32         %f_eta2,     %f_eta, %f_eta;

    // log_w = -0.9189385 + (-0.5)*eta² + (-0.5)*h
    mov.f32         %f_log2pi,   0fBF6B3F8E;       // -0.9189385f
    mov.f32         %f_neg_half, 0fBF000000;
    fma.rn.f32      %f_log_w,    %f_neg_half, %f_eta2, %f_log2pi;
    fma.rn.f32      %f_log_w,    %f_neg_half, %f_hi,   %f_log_w;

    // Store log_w[i]
    add.u64         %rd_lw,      %rd_lw, %rd_off;
    st.global.f32   [%rd_lw],    %f_log_w;

PROP_EXIT:
    ret;
}


// =============================================================================
// Kernel 2: bpf_set_scalar
//
// *scalar = val   (single-thread, launched <<<1,1>>>)
// =============================================================================

.visible .entry bpf_set_scalar(
    .param .u64 param_ptr,
    .param .f32 param_val
)
{
    .reg .u64   %rd_ptr;
    .reg .f32   %f_val;

    ld.param.u64    %rd_ptr, [param_ptr];
    ld.param.f32    %f_val,  [param_val];
    st.global.f32   [%rd_ptr], %f_val;
    ret;
}


// =============================================================================
// Kernel 3: bpf_reduce_max
//
// Block-parallel max reduction using shared memory.
// Each block reduces its chunk, thread 0 does atomicMax to global output.
//
// Shared memory: extern, declared as raw .b8 with .align 4
// Size set at launch time (blockDim.x * 4 bytes).
// =============================================================================

// Extern shared memory -- declared at module scope, used by reduce_max and reduce_sum
.extern .shared .align 4 .b8 smem_raw[];

.visible .entry bpf_reduce_max(
    .param .u64 param_in,
    .param .u64 param_out,
    .param .s32 param_n
)
{
    .reg .u32   %r_tid, %idx, %r_ctaid;
    .reg .u32   %r_n, %r_s, %r_partner, %r_ntid;
    .reg .u64   %rd_in, %rd_out, %rd_off;
    .reg .u64   %rd_smem_base, %rd_smem, %rd_smem_partner;
    .reg .f32   %f_val, %f_other, %f_max;
    .reg .pred  %p_bound, %p_active, %p_tid0, %p_loop;

    ld.param.u64    %rd_in,   [param_in];
    ld.param.u64    %rd_out,  [param_out];
    ld.param.s32    %r_n,     [param_n];

    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    // Shared memory base address
    mov.u64         %rd_smem_base, smem_raw;

    // sdata[tid] = (idx < n) ? in[idx] : -1e30
    mov.f32         %f_val,   0fF49A4F80;          // ~ -1.0e30f
    setp.lt.u32     %p_bound, %idx, %r_n;
    @!%p_bound bra  RMAX_LOAD_DONE;

    mul.wide.u32    %rd_off, %idx, 4;
    .reg .u64 %rd_src;
    add.u64         %rd_src, %rd_in, %rd_off;
    ld.global.f32   %f_val,  [%rd_src];

RMAX_LOAD_DONE:
    // sdata[tid] = f_val
    .reg .u64 %rd_tid_off;
    mul.wide.u32    %rd_tid_off, %r_tid, 4;
    add.u64         %rd_smem,    %rd_smem_base, %rd_tid_off;
    st.shared.f32   [%rd_smem],  %f_val;

    bar.sync        0;

    // Tree reduction: for (s = blockDim.x/2; s > 0; s >>= 1)
    shr.u32         %r_s, %r_ntid, 1;

REDUCE_MAX_LOOP:
    setp.eq.u32     %p_loop, %r_s, 0;
    @%p_loop bra    REDUCE_MAX_DONE;

    setp.lt.u32     %p_active, %r_tid, %r_s;
    @!%p_active bra REDUCE_MAX_SKIP;

    add.u32         %r_partner, %r_tid, %r_s;
    mul.wide.u32    %rd_off,    %r_partner, 4;
    add.u64         %rd_smem_partner, %rd_smem_base, %rd_off;
    ld.shared.f32   %f_other,   [%rd_smem_partner];
    ld.shared.f32   %f_val,     [%rd_smem];
    max.f32         %f_max,     %f_val, %f_other;
    st.shared.f32   [%rd_smem], %f_max;

REDUCE_MAX_SKIP:
    bar.sync        0;
    shr.u32         %r_s, %r_s, 1;
    bra             REDUCE_MAX_LOOP;

REDUCE_MAX_DONE:
    // Thread 0: atomicMax float via CAS loop
    setp.eq.u32     %p_tid0, %r_tid, 0;
    @!%p_tid0 bra   REDUCE_MAX_EXIT;

    ld.shared.f32   %f_val,  [%rd_smem];

    .reg .u32 %r_old, %r_assumed, %r_new;
    .reg .f32 %f_old, %f_cmp;
    .reg .pred %p_cas;

    ld.global.u32   %r_old, [%rd_out];

ATOMIC_MAX_LOOP:
    mov.u32         %r_assumed, %r_old;
    mov.b32         %f_old,     %r_assumed;
    max.f32         %f_cmp,     %f_val, %f_old;
    mov.b32         %r_new,     %f_cmp;
    atom.global.cas.b32 %r_old, [%rd_out], %r_assumed, %r_new;
    setp.ne.u32     %p_cas,     %r_old, %r_assumed;
    @%p_cas bra     ATOMIC_MAX_LOOP;

REDUCE_MAX_EXIT:
    ret;
}


// =============================================================================
// Kernel 4: bpf_reduce_sum
//
// Same structure as reduce_max: add instead of max, native atomicAdd.f32
// =============================================================================

.visible .entry bpf_reduce_sum(
    .param .u64 param_in,
    .param .u64 param_out,
    .param .s32 param_n
)
{
    .reg .u32   %r_tid, %idx, %r_ctaid;
    .reg .u32   %r_n, %r_s, %r_partner, %r_ntid;
    .reg .u64   %rd_in, %rd_out, %rd_off;
    .reg .u64   %rd_smem_base, %rd_smem, %rd_smem_partner;
    .reg .f32   %f_val, %f_other, %f_sum;
    .reg .pred  %p_bound, %p_active, %p_tid0, %p_loop;

    ld.param.u64    %rd_in,   [param_in];
    ld.param.u64    %rd_out,  [param_out];
    ld.param.s32    %r_n,     [param_n];

    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    mov.u64         %rd_smem_base, smem_raw;

    // sdata[tid] = (idx < n) ? in[idx] : 0.0
    mov.f32         %f_val,   0f00000000;
    setp.lt.u32     %p_bound, %idx, %r_n;
    @!%p_bound bra  RSUM_LOAD_DONE;

    mul.wide.u32    %rd_off, %idx, 4;
    .reg .u64 %rd_src;
    add.u64         %rd_src, %rd_in, %rd_off;
    ld.global.f32   %f_val,  [%rd_src];

RSUM_LOAD_DONE:
    .reg .u64 %rd_tid_off;
    mul.wide.u32    %rd_tid_off, %r_tid, 4;
    add.u64         %rd_smem,    %rd_smem_base, %rd_tid_off;
    st.shared.f32   [%rd_smem],  %f_val;

    bar.sync        0;

    shr.u32         %r_s, %r_ntid, 1;

REDUCE_SUM_LOOP:
    setp.eq.u32     %p_loop, %r_s, 0;
    @%p_loop bra    REDUCE_SUM_DONE;

    setp.lt.u32     %p_active, %r_tid, %r_s;
    @!%p_active bra REDUCE_SUM_SKIP;

    add.u32         %r_partner, %r_tid, %r_s;
    mul.wide.u32    %rd_off,    %r_partner, 4;
    add.u64         %rd_smem_partner, %rd_smem_base, %rd_off;
    ld.shared.f32   %f_other,   [%rd_smem_partner];
    ld.shared.f32   %f_val,     [%rd_smem];
    add.f32         %f_sum,     %f_val, %f_other;
    st.shared.f32   [%rd_smem], %f_sum;

REDUCE_SUM_SKIP:
    bar.sync        0;
    shr.u32         %r_s, %r_s, 1;
    bra             REDUCE_SUM_LOOP;

REDUCE_SUM_DONE:
    setp.eq.u32     %p_tid0, %r_tid, 0;
    @!%p_tid0 bra   REDUCE_SUM_EXIT;

    ld.shared.f32   %f_val, [%rd_smem];

    // Native float atomicAdd on SM_120
    atom.global.add.f32 %f_val, [%rd_out], %f_val;

REDUCE_SUM_EXIT:
    ret;
}


// =============================================================================
// Kernel 5: bpf_exp_sub
//
// w[i] = exp(log_w[i] - *d_max)
// exp(x) = exp2(x * log2(e))  -- this is what __expf() compiles to
// =============================================================================

.visible .entry bpf_exp_sub(
    .param .u64 param_w,
    .param .u64 param_log_w,
    .param .u64 param_d_max,
    .param .s32 param_n
)
{
    .reg .u32   %idx, %r_tid, %r_ctaid, %r_ntid;
    .reg .u32   %r_n;
    .reg .u64   %rd_w, %rd_lw, %rd_max, %rd_off;
    .reg .f32   %f_lw, %f_max, %f_diff, %f_exp;
    .reg .pred  %p_bound;

    ld.param.u64    %rd_w,    [param_w];
    ld.param.u64    %rd_lw,   [param_log_w];
    ld.param.u64    %rd_max,  [param_d_max];
    ld.param.s32    %r_n,     [param_n];

    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    setp.ge.u32     %p_bound, %idx, %r_n;
    @%p_bound bra   EXP_SUB_EXIT;

    mul.wide.u32    %rd_off,  %idx, 4;

    .reg .u64 %rd_src;
    add.u64         %rd_src,  %rd_lw, %rd_off;
    ld.global.f32   %f_lw,    [%rd_src];
    ld.global.f32   %f_max,   [%rd_max];

    sub.f32         %f_diff,  %f_lw, %f_max;

    // exp(diff) = exp2(diff * log2(e))
    mul.f32         %f_diff,  %f_diff, 0f3FB8AA3B;
    ex2.approx.f32  %f_exp,   %f_diff;

    .reg .u64 %rd_dst;
    add.u64         %rd_dst,  %rd_w, %rd_off;
    st.global.f32   [%rd_dst], %f_exp;

EXP_SUB_EXIT:
    ret;
}


// =============================================================================
// Kernel 6: bpf_scale_wh
//
// inv_sum = 1.0 / *d_sum       (rcp.approx -- hardware SFU reciprocal)
// w[i]  *= inv_sum
// wh[i]  = w[i] * h[i]
// =============================================================================

.visible .entry bpf_scale_wh(
    .param .u64 param_w,
    .param .u64 param_wh,
    .param .u64 param_h,
    .param .u64 param_d_sum,
    .param .s32 param_n
)
{
    .reg .u32   %idx, %r_tid, %r_ctaid, %r_ntid;
    .reg .u32   %r_n;
    .reg .u64   %rd_w, %rd_wh, %rd_h, %rd_sum, %rd_off;
    .reg .f32   %f_w, %f_h, %f_sum, %f_inv, %f_wh;
    .reg .pred  %p_bound;

    ld.param.u64    %rd_w,    [param_w];
    ld.param.u64    %rd_wh,   [param_wh];
    ld.param.u64    %rd_h,    [param_h];
    ld.param.u64    %rd_sum,  [param_d_sum];
    ld.param.s32    %r_n,     [param_n];

    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    setp.ge.u32     %p_bound, %idx, %r_n;
    @%p_bound bra   SCALE_EXIT;

    mul.wide.u32    %rd_off,  %idx, 4;

    .reg .u64 %rd_src_w, %rd_src_h;
    add.u64         %rd_src_w, %rd_w, %rd_off;
    add.u64         %rd_src_h, %rd_h, %rd_off;
    ld.global.f32   %f_w,      [%rd_src_w];
    ld.global.f32   %f_h,      [%rd_src_h];
    ld.global.f32   %f_sum,    [%rd_sum];

    rcp.approx.f32  %f_inv,    %f_sum;

    mul.f32         %f_w,      %f_w, %f_inv;
    st.global.f32   [%rd_src_w], %f_w;

    mul.f32         %f_wh,     %f_w, %f_h;
    .reg .u64 %rd_dst_wh;
    add.u64         %rd_dst_wh, %rd_wh, %rd_off;
    st.global.f32   [%rd_dst_wh], %f_wh;

SCALE_EXIT:
    ret;
}


// =============================================================================
// Kernel 7: bpf_compute_loglik
//
// d_scalars[3] = d_scalars[0] + log(max(d_scalars[1] / N, 1e-30))
// log(x) = lg2(x) * ln(2)
// =============================================================================

.visible .entry bpf_compute_loglik(
    .param .u64 param_scalars,
    .param .s32 param_n
)
{
    .reg .u64   %rd_s;
    .reg .f32   %f_max_lw, %f_sum_w, %f_n, %f_ratio;
    .reg .f32   %f_log2, %f_ln2, %f_loglik, %f_floor;
    .reg .s32   %r_n;

    ld.param.u64    %rd_s,      [param_scalars];
    ld.param.s32    %r_n,       [param_n];

    ld.global.f32   %f_max_lw,  [%rd_s + 0];
    ld.global.f32   %f_sum_w,   [%rd_s + 4];

    cvt.rn.f32.s32  %f_n,       %r_n;
    div.approx.f32  %f_ratio,   %f_sum_w, %f_n;

    // Clamp: max(ratio, 1e-30)
    mov.f32         %f_floor,   0f0D6BF94E;
    max.f32         %f_ratio,   %f_ratio, %f_floor;

    // log(ratio) = lg2(ratio) * ln(2),  ln(2) = 0.6931472f
    lg2.approx.f32  %f_log2,    %f_ratio;
    mov.f32         %f_ln2,     0f3F317218;
    mul.f32         %f_loglik,  %f_log2, %f_ln2;

    add.f32         %f_loglik,  %f_loglik, %f_max_lw;

    st.global.f32   [%rd_s + 12], %f_loglik;

    ret;
}


// =============================================================================
// Kernel 8: bpf_resample
//
// Systematic resampling via binary search on CDF.
//   target = u_base + i/N;  if target >= 1.0: target -= 1.0
//   binary search cdf[] -> ancestor index
//   h_out[i] = h_in[ancestor]
// =============================================================================

.visible .entry bpf_resample(
    .param .u64 param_h_out,
    .param .u64 param_h_in,
    .param .u64 param_cdf,
    .param .f32 param_u_base,
    .param .s32 param_n
)
{
    .reg .u32   %idx, %r_tid, %r_ctaid, %r_ntid;
    .reg .u32   %r_n;
    .reg .u64   %rd_out, %rd_in, %rd_cdf, %rd_off;
    .reg .f32   %f_u, %f_target, %f_idx_f, %f_n_f, %f_one, %f_cdf_mid;
    .reg .s32   %r_lo, %r_hi, %r_mid;
    .reg .pred  %p_bound, %p_ge1, %p_loop, %p_lt;

    ld.param.u64    %rd_out,  [param_h_out];
    ld.param.u64    %rd_in,   [param_h_in];
    ld.param.u64    %rd_cdf,  [param_cdf];
    ld.param.f32    %f_u,     [param_u_base];
    ld.param.s32    %r_n,     [param_n];

    mov.u32         %r_tid,   %tid.x;
    mov.u32         %r_ctaid, %ctaid.x;
    mov.u32         %r_ntid,  %ntid.x;
    mad.lo.u32      %idx,     %r_ctaid, %r_ntid, %r_tid;

    setp.ge.u32     %p_bound, %idx, %r_n;
    @%p_bound bra   RESAMPLE_EXIT;

    // target = u_base + (float)i / (float)n
    cvt.rn.f32.u32  %f_idx_f, %idx;
    cvt.rn.f32.s32  %f_n_f,   %r_n;
    div.approx.f32  %f_target, %f_idx_f, %f_n_f;
    add.f32         %f_target, %f_target, %f_u;

    // if target >= 1.0: target -= 1.0
    mov.f32         %f_one,    0f3F800000;
    setp.ge.f32     %p_ge1,    %f_target, %f_one;
    @%p_ge1 sub.f32 %f_target, %f_target, %f_one;

    // Binary search: lo=0, hi=n-1
    mov.s32         %r_lo,     0;
    sub.s32         %r_hi,     %r_n, 1;

BSEARCH_LOOP:
    setp.ge.s32     %p_loop,   %r_lo, %r_hi;
    @%p_loop bra    BSEARCH_DONE;

    // mid = (lo + hi) >> 1
    add.s32         %r_mid,    %r_lo, %r_hi;
    shr.s32         %r_mid,    %r_mid, 1;

    // Load cdf[mid]
    .reg .u64 %rd_cdf_mid;
    mul.wide.s32    %rd_off,   %r_mid, 4;
    add.u64         %rd_cdf_mid, %rd_cdf, %rd_off;
    ld.global.f32   %f_cdf_mid, [%rd_cdf_mid];

    // if cdf[mid] < target: lo = mid + 1, else: hi = mid
    setp.lt.f32     %p_lt,     %f_cdf_mid, %f_target;
    @%p_lt  add.s32 %r_lo,     %r_mid, 1;
    @!%p_lt mov.s32 %r_hi,     %r_mid;

    bra             BSEARCH_LOOP;

BSEARCH_DONE:
    // h_out[idx] = h_in[lo]
    .reg .u64 %rd_src, %rd_dst;
    .reg .f32 %f_h;

    mul.wide.s32    %rd_off,   %r_lo, 4;
    add.u64         %rd_src,   %rd_in, %rd_off;
    ld.global.f32   %f_h,      [%rd_src];

    mul.wide.u32    %rd_off,   %idx, 4;
    add.u64         %rd_dst,   %rd_out, %rd_off;
    st.global.f32   [%rd_dst], %f_h;

RESAMPLE_EXIT:
    ret;
}
